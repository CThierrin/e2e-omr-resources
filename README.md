# e2e-omr-resources

For running an end-to-end OMR workflow, you need the resources indicated in the [end-to-end OMR documentation](http://ddmal.music.mcgill.ca/e2e-omr-documentation/#digital-resources). In this repository you can find these resources, some are applicable to different manuscripts, while others are specificly for the Salzinnes Antiphonal:
-	The *document analysis* or *document segmentation* models. These models are used to segment the image of a manuscript page into different layers (e.g., music symbol layer, staff-line layer, and background layer). The models are trained to identify the pixels belonging to a particular layer. The three models: music-symbol model, staff-line model, and background model, trained for the Salzinnes manuscript can be found [`document_analysis/Salzinnes/models`](https://github.com/DDMAL/e2e-omr-resources/blob/main/document_analysis/Salzinnes/models/). These models were generated by the `Training model for Patchwise Analysis of Music Document - HPC` job, using the images provided in the folder [`document_analysis/Salzinnes/training_data`](https://github.com/DDMAL/e2e-omr-resources/tree/main/document_analysis/Salzinnes/training_data). 

    There is also a non-HPC version of the job. Depending on the number of resources needed, you would use the HPC or non-HPC version. HPC is meant to be used when a large amount of memory is needed. 
    
    For information about the settings of the training job used to generate these models for Salzinnes and other details, please consult the [`document_analysis/Salzinnes`](https://github.com/DDMAL/e2e-omr-resources/tree/main/document_analysis/Salzinnes/) directory.

-	For processing the music symbol layer and successfully identify the individual symbols, you also need two files to provide to the interactive (or non-interactive) classifier:
    -	the *training data* for classifying music symbols: `music_symbol_recognition/split_training.xml`
    -	the *feature selection/weights* to optimize such classification process: `music_symbol_recognition/split_features.xml`

-	For the processing of the text (extracted from the background layer), you need two files:
    - the *OCR model*: `text_alignment/salzinnes_model-00054500.pyrnn`
    - the actual *transcript text* for each page of the manuscript. The text can be obtained from the Cantus Database. See [instructions](http://ddmal.music.mcgill.ca/e2e-omr-documentation/tutorial/music-reconstruction.html#text-alignment) on how to retrieve a CSV file including the text of the complete manuscript, and how to identify the rows in the file that relate to the text in a particular page. You can find the extracted texts for Salzinnes in `text_alignment/Salzinnes/`, and the python script used to retrieve these text files in `text_alignment/get_text_per_folio.py`

-	Finally, to encode everything together in an MEI file, you also need to provide a *CSV file mapping* the classes of glyphs to fragments of MEI code. This file can be obtained through CRES. For square neume notation, you could use the `mei_encoding/csv-square_notation_test.csv` file.
